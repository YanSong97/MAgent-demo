{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAgent repo: https://github.com/PettingZoo-Team/MAgent\n",
    "\n",
    "Installment:  pip install magent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import magent\n",
    "from MAgent.examples.models import buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magent.gridworld import Config   #used to cutomise gridworld env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setting up env ('pursuit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "map_size = 50\n",
    "env_name = \"pursuit\"\n",
    "env = magent.GridWorld(env_name, map_size=map_size)    #setup pursuit env\n",
    "env.reset()                                             #reset env , i.e. agent group\n",
    "handles = env.get_handles()                             #list of agent groups, which contains 'predator' and 'prey' in pursuit game\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Default config of env:\n",
    "\n",
    "Env name      |     handles     |  handles setting\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "pursuit       | [predator,prey] | predator{'width': 2, 'length': 2, 'hp': 1, 'speed': 1, 'view_range': 5, 'attack range': 2, 'attack penality':-0.2}\n",
    "              |                 | prey    {'width': 1, 'length': 1, 'hp': 1, 'speed': 1.5,'view range': 4, 'attack range':0}\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "battle        | [army1, army2]  | army: {width': 1, 'length': 1, 'hp': 10, 'speed': 2, 'view range':6,'attack range':1.5, 'damage': 2\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "double attack | [deer, tiger]   | deer{'width': 1, 'length': 1, 'hp': 5, 'speed': 1,'view range':1, 'attack range':0}\n",
    "              |                 | tiger{'width': 1, 'length': 1, 'hp': 10, 'speed': 1, 'view range':4, 'attack range':1}\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "forest        | [deer, tiger]   | deer{'width': 1, 'length': 1, 'hp': 5, 'speed': 1,'view range':1, 'attack range':0}\n",
    "              |                 | tiger{'width': 1, 'length': 1, 'hp': 10, 'speed': 1, 'view range':4, 'attack range':1}\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "One may also customise the env sertting by sending Config: env = magent.GridWorld(config)\n",
    "and defining own reward fn :\n",
    "----------------------------------------------------------------\n",
    "a = AgentSymbol(tiger_group, index='any')\n",
    "b = AgentSymbol(tiger_group, index='any')\n",
    "c = AgentSymbol(deer_group,  index='any')\n",
    "\n",
    "# tigers get reward when they attack a deer simultaneously\n",
    "e1 = Event(a, 'attack', c)\n",
    "e2 = Event(b, 'attack', c)\n",
    "config.add_reward_rule(e1 & e2, receiver=[a, b], value=[1, 1])\n",
    "----------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "Env observation:\n",
    "\n",
    "There are two parts in observation, spacial local view and non-spacial feature.(for detailed see https://github.com/geek-ai/MAgent/blob/master/doc/get_started.md)\n",
    "\n",
    "Spatial view consists of several rectangular channels. These channels will be masked by a circle or a sector \n",
    "as described in the agent type registration. If the radius of a circle is 5, then the size of one channel \n",
    "is 11 x 11, where 11 = 5x2 + 1\n",
    "\n",
    "Non-spatial feature includes ID embedding, last action, last reward and normalized position. \n",
    "ID embedding is the binary representation of agent's unique ID.\n",
    "\n",
    "\n",
    "Env action:\n",
    "Actions are discrete actions. They can be move, attack and turn\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print('')\n",
    "#some config of env\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env info:\n",
      "view space: {0: (10, 10, 5), 1: (9, 9, 5)};\n",
      "feature space: {0: (14,), 1: (10,)};\n",
      "action space: {0: (13,), 1: (9,)}\n",
      "\n",
      "Agent group size:\n",
      "Number of predators: 15; Number of preys: 15\n",
      "\n",
      "Agent info:\n",
      "predator position shape: (15, 2); prey position shape: (15, 2)\n",
      "predator view shape: (15, 10, 10, 5); prey view shape: (15, 9, 9, 5)\n",
      "predator feature shape: (15, 14); prey feature shape: (15, 10)\n",
      "predator reward shape:(15,); prey reward shape:(15,)\n",
      "predator alive shape: (15,); prey alive shape: (15,)\n"
     ]
    }
   ],
   "source": [
    "env.reset()                                             #reset env , i.e. agent group\n",
    "handles = env.get_handles()  \n",
    "\n",
    "\n",
    "\n",
    "#One can get access to the env setting by\n",
    "view_space = env.view_space             #what the agents see, (partially-obsersed env)\n",
    "feature_space = env.feature_space       #information embedding\n",
    "action_space = env.action_space         #how the agents act\n",
    "print('Env info:')\n",
    "print('view space: {};\\nfeature space: {};\\naction space: {}\\n'.format(view_space,\n",
    "                                                                    feature_space,\n",
    "                                                                    action_space))\n",
    "#or the setting for each agent group individually by\n",
    "view_space1, view_space2 = env.get_view_space(handles[0]),env.get_view_space(handles[1])\n",
    "featuer_space1, feature_space2 = env.get_feature_space(handles[0]), env.get_feature_space(handles[1])\n",
    "action_space1, action_space2 = env.get_action_space(handles[0]), env.get_action_space(handles[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#then one can start adding agent to each agent group(handle) by\n",
    "num_agents = 15\n",
    "env.add_agents(handles[0], method = 'random', n = num_agents)    \n",
    "         #add num_agents number of agents randomly to the env and assign them to group handles[0]\n",
    "         #one can aslo customise the postion (see fn generate_map() )\n",
    "env.add_agents(handles[1], method = 'random', n = num_agents)           #one can also add obstacle such as walls\n",
    "        \n",
    "predator_num, prey_num = env.get_num(handles[0]), env.get_num(handles[1])     #ask for the size of each agent group\n",
    "print('Agent group size:')\n",
    "print('Number of predators: {}; Number of preys: {}\\n'.format(predator_num, prey_num))\n",
    "\n",
    "predator_ids, prey_ids = env.get_agent_id(handles[0]), env.get_agent_id(handles[1]) #ask for the agent id list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Once agents have beed added, one can ask for the detailed info of each single agent at current state by\n",
    "predator_position, prey_position = env.get_pos(handles[0]), env.get_pos(handles[1])   #[agent_num, 2]\n",
    "\n",
    "predator_view, predator_feature = env.get_observation(handles[0])   #[num_agent, view_space/feature_space]\n",
    "prey_view, prey_feature = env.get_observation(handles[1])       \n",
    "\n",
    "predator_reward = env.get_reward(handles[0])   #(agent_num, )\n",
    "prey_reward = env.get_reward(handles[1])\n",
    "\n",
    "predator_alive, prey_alive = env.get_alive(handles[0]), env.get_alive(handles[1])\n",
    "\n",
    "print('Agent info:')\n",
    "print('predator position shape: {}; prey position shape: {}'.format(predator_position.shape, prey_position.shape))\n",
    "print('predator view shape: {}; prey view shape: {}'.format(predator_view.shape, prey_view.shape))\n",
    "print('predator feature shape: {}; prey feature shape: {}'.format(predator_feature.shape, prey_feature.shape))\n",
    "print('predator reward shape:{}; prey reward shape:{}'.format(predator_reward.shape, prey_reward.shape))\n",
    "print('predator alive shape: {}; prey alive shape: {}'.format(predator_alive.shape, prey_alive.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_map(env, map_size, handles):\n",
    "    \"\"\" customise a map, which consists of two squares of agents\"\"\"\n",
    "    width = height = map_size\n",
    "    init_num = map_size * map_size * 0.04\n",
    "    gap = 3\n",
    "\n",
    "    # left\n",
    "    n = init_num\n",
    "    side = int(math.sqrt(n)) * 2\n",
    "    pos = []\n",
    "    for x in range(width//2 - gap - side, width//2 - gap - side + side, 2):\n",
    "        for y in range((height - side)//2, (height - side)//2 + side, 2):\n",
    "            pos.append([x, y, 0])\n",
    "    env.add_agents(handles[0], method=\"custom\", pos=pos)\n",
    "\n",
    "    # right\n",
    "    n = init_num\n",
    "    side = int(math.sqrt(n)) * 2\n",
    "    pos = []\n",
    "    for x in range(width//2 + gap, width//2 + gap + side, 2):\n",
    "        for y in range((height - side)//2, (height - side)//2 + side, 2):\n",
    "            pos.append([x, y, 0])\n",
    "    env.add_agents(handles[1], method=\"custom\", pos=pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_predator = env.get_global_minimap(50,50)[:,:,0]\n",
    "fig_prey = env.get_global_minimap(50,50)[:,:,1]\n",
    "\n",
    "plt.imshow(fig_predator - fig_prey, cmap = 'Greys');   #\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A testing framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   0,  nums: [15, 15] reward: [-2.2  0. ],  total_reward: [-2.2  0. ] \n",
      "step  50,  nums: [15, 15] reward: [-1.4  0. ],  total_reward: [-86.2  -3. ] \n",
      "step 100,  nums: [15, 15] reward: [-2.  0.],  total_reward: [-177.   -5.] \n",
      "step 150,  nums: [15, 15] reward: [-2.  0.],  total_reward: [-263.4   -6. ] \n",
      "step 200,  nums: [15, 15] reward: [-1.8  0. ],  total_reward: [-352.4   -8. ] \n",
      "step 250,  nums: [15, 15] reward: [-2.4  0. ],  total_reward: [-436.4  -14. ] \n",
      "step 300,  nums: [15, 15] reward: [-1.8  0. ],  total_reward: [-522.8  -16. ] \n",
      "step 350,  nums: [15, 15] reward: [-1.  0.],  total_reward: [-603.4  -23. ] \n",
      "step 400,  nums: [15, 15] reward: [-1.8  0. ],  total_reward: [-695.6  -28. ] \n",
      "step 450,  nums: [15, 15] reward: [-2.4  0. ],  total_reward: [-790.8  -32. ] \n",
      "step 500,  nums: [15, 15] reward: [-2.2  0. ],  total_reward: [-876.2  -35. ] \n",
      "step 550,  nums: [15, 15] reward: [-2.2  0. ],  total_reward: [-960.  -42.] \n"
     ]
    }
   ],
   "source": [
    "print_every = 50\n",
    "\n",
    "\n",
    "env.reset()  \n",
    "handles = env.get_handles()\n",
    "num_agents = 15\n",
    "env.add_agents(handles[0], method = 'random', n = num_agents)    \n",
    "env.add_agents(handles[1], method = 'random', n = num_agents)\n",
    "\n",
    "step_ct = 0\n",
    "done = False\n",
    "\n",
    "\n",
    "n = len(handles)                                     #number of agent groups, 2 in this case\n",
    "obs  = [[] for _ in range(n)]   #observation container\n",
    "ids  = [[] for _ in range(n)]   #agent id container\n",
    "acts = [[] for _ in range(n)]   #action container\n",
    "nums = [env.get_num(handle) for handle in handles]   #[num of predator, num of prey]\n",
    "sample_buffer = [buffer.EpisodesBuffer(capacity=1500) for _ in range(n)]\n",
    "\n",
    "total_reward = [0 for _ in range(n)]\n",
    "\n",
    "\n",
    "while not done:\n",
    "    # take actions for every model\n",
    "    for i in range(n):\n",
    "        obs[i] = env.get_observation(handles[i])   #ask for observation, the view shape [num_agents, ...]\n",
    "        ids[i] = env.get_agent_id(handles[i])       #ask for agent's id\n",
    "        acts[i] =  np.int32(np.random.choice(range(env.get_action_space(handles[0])[0]), env.get_num(handles[0])))\n",
    "                 #random action\n",
    "        env.set_action(handles[i], acts[i])      #send the action to the env\n",
    "\n",
    "    # simulate one step\n",
    "    done = env.step()\n",
    "\n",
    "    # sample\n",
    "    step_reward = []\n",
    "    for i in range(n):\n",
    "        rewards = env.get_reward(handles[i])     #ask for the reawrd\n",
    "        #if train:\n",
    "        alives = env.get_alive(handles[i])  #ask whether the agent stay alive\n",
    "        sample_buffer[i].record_step(ids[i], obs[i], acts[i], rewards, alives)\n",
    "        \n",
    "        s = sum(rewards)\n",
    "        step_reward.append(s)\n",
    "        total_reward[i] += s\n",
    "\n",
    "\n",
    "    # stat info\n",
    "    nums = [env.get_num(handle) for handle in handles]\n",
    "\n",
    "    # clear dead agents\n",
    "    env.clear_dead()\n",
    "\n",
    "    if step_ct % print_every == 0:\n",
    "        print(\"step %3d,  nums: %s reward: %s,  total_reward: %s \" %\n",
    "              (step_ct, nums, np.around(step_reward, 2), np.around(total_reward, 2)))\n",
    "    step_ct += 1\n",
    "    if step_ct > 550:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A training framwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import torch.optim as optimizer\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"a circular queue based on numpy array, supporting batch put and batch get\"\"\"\n",
    "    def __init__(self, shape, dtype=np.float32):\n",
    "        self.buffer = np.empty(shape=shape, dtype=dtype)\n",
    "        self.head   = 0\n",
    "        self.capacity   = len(self.buffer)\n",
    "\n",
    "    def put(self, data):\n",
    "        \"\"\"put data to\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data: numpy array\n",
    "            data to add\n",
    "        \"\"\"\n",
    "        head = self.head\n",
    "        n = len(data)\n",
    "        if head + n <= self.capacity:\n",
    "            self.buffer[head:head+n] = data\n",
    "            self.head = (self.head + n) % self.capacity\n",
    "        else:\n",
    "            split = self.capacity - head\n",
    "            self.buffer[head:] = data[:split]\n",
    "            self.buffer[:n - split] = data[split:]\n",
    "            self.head = split\n",
    "        return n\n",
    "\n",
    "    def get(self, index):\n",
    "        \"\"\"get items\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        index: int or numpy array\n",
    "            it can be any numpy supported index\n",
    "        \"\"\"\n",
    "        return self.buffer[index]\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"clear replay buffer\"\"\"\n",
    "        self.head = 0\n",
    "\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class CNN_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    view size : [batch, 13, 13,7]\n",
    "    feature size: [batch, 34]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNN_encoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 5, out_channels = 32, kernel_size = 3, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "    \n",
    "    def forward(self, view_state):\n",
    "        \"\"\"\n",
    "        [batch, 13,13,7]\n",
    "        \"\"\"\n",
    "        state = view_state.permute(0,3,1,2)\n",
    "        \n",
    "        return self.net(state)  #[batch, 128]\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, output_size, input_size=128, hidden_size=128, feature_size = 14):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = CNN_encoder()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.view_linear = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        self.feature_linear = nn.Linear(feature_size, feature_size)\n",
    "        \n",
    "        self.concat_linear = nn.Sequential(\n",
    "            nn.Linear(hidden_size+feature_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, view_state, feature_state):\n",
    "        \"\"\"\n",
    "        view_state: [batch, 13,13,7]\n",
    "        feature_state: [batch, 34]\n",
    "        \"\"\"\n",
    "        view_encoded_state = self.view_linear(self.conv(view_state))\n",
    "        view_encoded_state = F.relu(view_encoded_state)\n",
    "\n",
    "        feature_encoded_state = F.relu(self.feature_linear(feature_state))\n",
    "        \n",
    "        concate = torch.cat([view_encoded_state, feature_encoded_state], dim = -1)  #[]batch, hidden+feature\n",
    "        \n",
    "        return self.concat_linear(concate)   #[batch, output_size]\n",
    "    \n",
    "        \n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self, action_dim, env, handle, memory_size):\n",
    "        \"\"\"\n",
    "        also input env for parameter enquiry, and handle\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        #self.hidden_size = args.hidden_size\n",
    "        self.lr = 0.001 #args.c_lr\n",
    "        #self.buffer_size = args.buffer_capacity\n",
    "        #self.batch_size = args.batch_size\n",
    "        #self.gamma = args.gamma\n",
    "        self.batch_size = 50\n",
    "        self.train_freq = 1\n",
    "        \n",
    "        \n",
    "        self.view_space = env.get_view_space(handle)   #[13,13,7]\n",
    "        self.feature_space = env.get_feature_space(handle)  #[34, ]\n",
    "        self.action_space = env.get_action_space(handle)  #[21,]\n",
    "        self.env = env\n",
    "        self.handle = handle\n",
    "        \n",
    "        self.critic_eval = Critic(output_size = action_dim, feature_size = self.feature_space[0])\n",
    "        self.critic_target = Critic(output_size = action_dim, feature_size = self.feature_space[0])\n",
    "        self.optimizer = optimizer.Adam(self.critic_eval.parameters(), lr=self.lr)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.replay_buf_len = 0          #live recording the length of replay buffer                                                    \n",
    "        self.memory_size = memory_size                                                     \n",
    "        self.replay_buf_view     = ReplayBuffer(shape=(memory_size,) + self.view_space)    # (memory, 13,13,7)\n",
    "        self.replay_buf_feature  = ReplayBuffer(shape=(memory_size,) + self.feature_space) \n",
    "        self.replay_buf_action   = ReplayBuffer(shape=(memory_size,), dtype=np.int32)      \n",
    "        self.replay_buf_reward   = ReplayBuffer(shape=(memory_size,))                      \n",
    "        self.replay_buf_terminal = ReplayBuffer(shape=(memory_size,), dtype=np.bool)       \n",
    "        self.replay_buf_mask     = ReplayBuffer(shape=(memory_size,))                      \n",
    "        \n",
    "        \n",
    "\n",
    "        # exploration\n",
    "        #self.eps = args.epsilon\n",
    "        #self.eps_end = args.epsilon_end\n",
    "        #self.eps_delay = 1 / (args.max_episode * 100)\n",
    "\n",
    "        # 更新target网\n",
    "        self.learn_step_counter = 0\n",
    "        self.target_replace_iter = 50#args.target_replace\n",
    "        self.target_update = 1\n",
    "\n",
    "        #trajectory_property = get_trajectory_property()\n",
    "        #self.memory = buffer(self.buffer_size, trajectory_property)\n",
    "        #self.memory.init_item_buffers()\n",
    "        \n",
    "    def choose_action(self, raw_obs, ids, greedy):\n",
    "        \"\"\"\n",
    "        raw_obs: tuple([total_num, 13, 13, 7], [total_num, feature_size]), np array\n",
    "        ids:   numpy array ,ids of agents\n",
    "        greedy: if greedy, all agent act greedily, otherwise ([0,1] float) act e-greedy\n",
    "        need to batchify the data \n",
    "        \"\"\"\n",
    "        view, feature = raw_obs[0], raw_obs[1]\n",
    "        view = torch.FloatTensor(view)\n",
    "        feature = torch.FloatTensor(feature)\n",
    "        \n",
    "        n = len(view)  #total number of data\n",
    "        batch_size = min(n, self.batch_size)\n",
    "        \n",
    "        ret = []\n",
    "        for i in range(0, n, batch_size):\n",
    "            beg, end = i, i+batch_size\n",
    "\n",
    "            ret.append(self.critic_eval(view[beg:end], feature[beg:end]).detach().cpu().numpy())\n",
    "        ret = np.concatenate(ret)   #[total_num, action_dim]\n",
    "        \n",
    "        #sampled action\n",
    "            \n",
    "        greedy_action = np.argmax(ret, axis = -1)   #[total_num, ]\n",
    "        \n",
    "        if greedy is True:\n",
    "            return np.int32(greedy_action)\n",
    "        else:\n",
    "            random_action = np.random.choice(range(self.action_dim), n)\n",
    "            indicator = (torch.rand(n).uniform_(0,1) < greedy).float().numpy()\n",
    "            \n",
    "            action = greedy_action * np.abs(1-indicator) + random_action * indicator\n",
    "            return np.int32(action)\n",
    "            \n",
    "\n",
    "    def add_replay_buffer(self, memory):\n",
    "        \n",
    "        n = 0\n",
    "        for episode in memory.episodes():   #contain data for each episodic run for each single agent\n",
    "            v, f, a, r = episode.views, episode.features, episode.actions, episode.rewards   \n",
    "            m = len(r)   #length of the episode\n",
    "            \n",
    "            mask = np.ones((m,))\n",
    "            terminal = np.full((m,), False)  #np.zeros((m,), dtype = np.bool)\n",
    "            if episode.terminal:\n",
    "                terminal[-1] = True\n",
    "            else:\n",
    "                mask[-1] = 0    #end the episode if its not terminated\n",
    "            \n",
    "            self.replay_buf_view.put(v)\n",
    "            \n",
    "            self.replay_buf_feature.put(f)                                                   \n",
    "            self.replay_buf_action.put(a)                                                    \n",
    "            self.replay_buf_reward.put(r)                                                    \n",
    "            self.replay_buf_terminal.put(terminal)                                           \n",
    "            self.replay_buf_mask.put(mask)  \n",
    "            \n",
    "            n += m\n",
    "            \n",
    "        self.replay_buf_len = min(self.memory_size, self.replay_buf_len + n)      #update buffer length\n",
    "        \n",
    "        return n\n",
    "\n",
    "    \n",
    "    def learn(self, sample_buffer):\n",
    "        add_num = self.add_replay_buffer(sample_buffer)\n",
    "        batch_size = self.batch_size\n",
    "        total_loss = 0\n",
    "        \n",
    "        n_batches = int(self.train_freq * add_num / batch_size)  \n",
    "        print(\"batch number: %d  add: %d  replay_len: %d/%d\" %               \n",
    "              (n_batches, add_num, self.replay_buf_len, self.memory_size))   \n",
    "        \n",
    "        start_time = time.time()  \n",
    "        ct = 0            \n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            index = np.random.choice(self.replay_buf_len - 1, batch_size)\n",
    "            \n",
    "            batch_view     = torch.FloatTensor(self.replay_buf_view.get(index))             #[batch, 13,13,7]                      \n",
    "            batch_feature  = torch.FloatTensor(self.replay_buf_feature.get(index))          #[batch, feature_dim]                       \n",
    "            batch_action   = torch.FloatTensor((self.replay_buf_action.get(index))).unsqueeze(-1)  #[batch, 1]                                 \n",
    "            batch_reward   = torch.FloatTensor(self.replay_buf_reward.get(index)).unsqueeze(-1)  #[batch, 1]                                 \n",
    "            batch_terminal = self.replay_buf_terminal.get(index)                               \n",
    "            batch_mask     = torch.FloatTensor(self.replay_buf_mask.get(index)).unsqueeze(-1)   \n",
    "            \n",
    "            batch_next_view = torch.FloatTensor(self.replay_buf_view.get(index+1))\n",
    "            batch_next_feature = torch.FloatTensor(self.replay_buf_feature.get(index+1))\n",
    "\n",
    "            batch_current_q = self.critic_eval(batch_view, batch_feature).gather(1, batch_action.long())\n",
    "            batch_next_q = self.critic_target(batch_next_view, batch_next_feature).detach()\n",
    "            batch_q_target = batch_reward + batch_mask * self.gamma * (batch_next_q.max(dim = -1)[0]).unsqueeze(-1)\n",
    "            loss_fn = nn.MSELoss()\n",
    "            loss = loss_fn(batch_current_q, batch_q_target)\n",
    "            \n",
    "            total_loss += loss\n",
    "        \n",
    "        if ct % self.target_update == 0:      \n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            print('Update Q')\n",
    "            \n",
    "        if self.learn_step_counter % self.target_replace_iter == 0:\n",
    "            self.critic_target.load_state_dict(self.critic_eval.state_dict())\n",
    "            print('update target Q')\n",
    "        self.learn_step_counter += 1    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train (1 episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step   0,  nums: [15, 15] reward: [0. 0.],  total_reward: [0. 0.] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yansong/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:154: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  50,  nums: [15, 15] reward: [-0.6  0. ],  total_reward: [-19.2  -3. ] \n",
      "step 100,  nums: [15, 15] reward: [ 1.6 -2. ],  total_reward: [-26.6 -12. ] \n",
      "step 150,  nums: [15, 15] reward: [-0.8  0. ],  total_reward: [-34.6 -22. ] \n",
      "step 200,  nums: [15, 15] reward: [-0.4  0. ],  total_reward: [-38.6 -39. ] \n",
      "step 250,  nums: [15, 15] reward: [-0.2  0. ],  total_reward: [-55.6 -42. ] \n",
      "step 300,  nums: [15, 15] reward: [0. 0.],  total_reward: [-60.8 -52. ] \n",
      "step 350,  nums: [15, 15] reward: [-0.2  0. ],  total_reward: [-65.2 -67. ] \n",
      "step 400,  nums: [15, 15] reward: [-0.4  0. ],  total_reward: [-67.8 -82. ] \n",
      "step 450,  nums: [15, 15] reward: [0. 0.],  total_reward: [-74.4 -92. ] \n",
      "step 500,  nums: [15, 15] reward: [-0.2  0. ],  total_reward: [-88.2 -95. ] \n",
      "step 550,  nums: [15, 15] reward: [-0.2  0. ],  total_reward: [ -97.8 -103. ] \n",
      "batch number: 165  add: 8265  replay_len: 1024/1024\n",
      "Update Q\n",
      "update target Q\n",
      "batch number: 165  add: 8265  replay_len: 1024/1024\n",
      "Update Q\n",
      "update target Q\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "env.reset()  \n",
    "handles = env.get_handles()\n",
    "num_agents = 15\n",
    "env.add_agents(handles[0], method = 'random', n = num_agents)    \n",
    "env.add_agents(handles[1], method = 'random', n = num_agents)\n",
    "\n",
    "step_ct = 0\n",
    "done = False\n",
    "\n",
    "\n",
    "n = len(handles)                                     #number of agent groups, 2 in this case\n",
    "obs  = [[] for _ in range(n)]   #observation container\n",
    "ids  = [[] for _ in range(n)]   #agent id container\n",
    "acts = [[] for _ in range(n)]   #action container\n",
    "nums = [env.get_num(handle) for handle in handles]   #[num of predator, num of prey]\n",
    "sample_buffer = [buffer.EpisodesBuffer(capacity=15000) for _ in range(n)]\n",
    "#sample_buffer = buffer.EpisodesBuffer(capacity=15000)\n",
    "#sample_buffer2 = buffer.EpisodesBuffer(capacity=1500)\n",
    "#sample_buffer = [sample_buffer1, sample_buffer2]\n",
    "\n",
    "total_reward = [0 for _ in range(n)]\n",
    "\n",
    "models = []\n",
    "models.append(DQN(env.get_action_space(handles[0])[0], env, handles[0], 2**10))\n",
    "models.append(DQN(env.get_action_space(handles[1])[0], env, handles[1], 2**10))\n",
    "\n",
    "\n",
    "while not done:\n",
    "    # take actions for every model\n",
    "    for i in range(n):\n",
    "        obs[i] = env.get_observation(handles[i])   #ask for observation, the view shape [num_agents, ...]\n",
    "        ids[i] = env.get_agent_id(handles[i])       #ask for agent's id\n",
    "        acts[i] = models[i].choose_action(obs[i], ids[i], 0.2)\n",
    "                 #random action\n",
    "        env.set_action(handles[i], acts[i])      #send the action to the env\n",
    "\n",
    "    # simulate one step\n",
    "    done = env.step()\n",
    "\n",
    "    # sample\n",
    "    step_reward = []\n",
    "    for i in range(n):\n",
    "        rewards = env.get_reward(handles[i])     #ask for the reawrd\n",
    "        #if train:\n",
    "        alives = env.get_alive(handles[i])  #ask whether the agent stay alive\n",
    "        sample_buffer[i].record_step(ids[i], obs[i], acts[i], rewards, alives)\n",
    "        \n",
    "        s = sum(rewards)\n",
    "        step_reward.append(s)\n",
    "        total_reward[i] += s\n",
    "\n",
    "\n",
    "    # stat info\n",
    "    nums = [env.get_num(handle) for handle in handles]\n",
    "\n",
    "    # clear dead agents\n",
    "    env.clear_dead()\n",
    "\n",
    "    if step_ct % print_every == 0:\n",
    "        print(\"step %3d,  nums: %s reward: %s,  total_reward: %s \" %\n",
    "              (step_ct, nums, np.around(step_reward, 2), np.around(total_reward, 2)))\n",
    "    step_ct += 1\n",
    "    if step_ct > 550:\n",
    "        break\n",
    "\n",
    "for i in range(n):\n",
    "    models[i].learn(sample_buffer[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
